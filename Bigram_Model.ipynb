{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import the librairies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "id": "tjrWhgLAgDgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug = True #activate if you want to compute the print/test to see if each step is correctly working\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "max_iters = 40000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "5vSmTn7BgqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48aa31c8-a351-4cef-e49e-4e396c9e0668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7927cff5f8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the data\n"
      ],
      "metadata": {
        "id": "FNDDzOU1bMp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn2Obi70aiv9",
        "outputId": "2c50d7cb-3ffa-462a-ef70-ef5c8f1d8272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-24 15:18:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-09-24 15:18:42 (16.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt #Shakespeare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "jlxPV34vamEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-7C4EIbcdi",
        "outputId": "7ccc5d7a-3165-4f1b-f126-f74c1752baa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iA2uHWtbi0P",
        "outputId": "9d8887ce-34c3-442b-f224-021c86a93cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a map between characters and integers\n",
        "**Objective**: associate to each characters to an integers using dictionnaries"
      ],
      "metadata": {
        "id": "esV1rfQncxLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_char = sorted(list(set(text)))\n",
        "vocab_size = len(list_char)\n",
        "if debug:\n",
        "  print(\"vocab_size :\", vocab_size)\n",
        "  print(list_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JewWArZibmzp",
        "outputId": "7d06b77c-ae45-4e82-be00-55b9164d398c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size : 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionnary char -> integer\n",
        "stoi = {c:i for i,c in enumerate(list_char)}\n",
        "itos = {i:c for i,c in enumerate(list_char)}\n",
        "\n",
        "if debug:\n",
        "  print(\"stoi :\", stoi)\n",
        "  print(\"itos :\", itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ULJXV_1d85m",
        "outputId": "f269fb77-79ab-45d6-ed29-09b2504db25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stoi : {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "itos : {0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to encode and decode string using dictionnaries\n",
        "\n",
        "def encode(text, stoi = stoi):\n",
        "  list_integers = []\n",
        "  for c in text:\n",
        "    list_integers.append(stoi.get(c))\n",
        "\n",
        "  return list_integers\n",
        "\n",
        "\n",
        "def decode(list_integers, itos=itos):\n",
        "  text = []\n",
        "  for i in list_integers:\n",
        "    text.append(itos.get(i))\n",
        "\n",
        "  text = ''.join(c for c in text) #delete this line if you want a list of char instead of a str\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "if debug:\n",
        "  print(encode(\"hello, I am doing a gpt model\"))\n",
        "  print(decode([5, 48, 19, 23]))\n",
        "  print(decode(encode(\"hello, I am doing a gpt model\")))\n",
        "\n",
        "  print(decode([19]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CGTPP34gSi-",
        "outputId": "bde0383d-175f-44bb-ee84-9d9d518632d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 6, 1, 21, 1, 39, 51, 1, 42, 53, 47, 52, 45, 1, 39, 1, 45, 54, 58, 1, 51, 53, 42, 43, 50]\n",
            "'jGK\n",
            "hello, I am doing a gpt model\n",
            "G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data and split it into training and validation set"
      ],
      "metadata": {
        "id": "c6wNxlkim7no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tok = encode(text)\n",
        "if debug : print(data_tok[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ_kNZ2rm7XO",
        "outputId": "da841fd2-3574-4a8e-b1f8-1941ceb059b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fraction_training_data = 0.9\n",
        "\n",
        "\n",
        "z = int(fraction_training_data*len(data_tok))\n",
        "train_set = data_tok[:z]\n",
        "validation_set = data_tok[z:]\n",
        "\n",
        "if debug:\n",
        "  print(\"train set -> length :\", len(train_set))\n",
        "  print(train_set[:100], \"\\n\")\n",
        "  print(\"validation set -> length :\", len(validation_set))\n",
        "  print(validation_set[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfaqDgini9i7",
        "outputId": "1d5ab7de-790b-4cc8-b34a-b4b920487188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set -> length : 1003854\n",
            "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59] \n",
            "\n",
            "validation set -> length : 111540\n",
            "[12, 0, 0, 19, 30, 17, 25, 21, 27, 10, 0, 19, 53, 53, 42, 1, 51, 53, 56, 56, 53, 61, 6, 1, 52, 43, 47, 45, 46, 40, 53, 59, 56, 1, 14, 39, 54, 58, 47, 57, 58, 39, 8, 0, 0, 14, 13, 28, 32, 21, 31, 32, 13, 10, 0, 19, 53, 53, 42, 1, 51, 53, 56, 56, 53, 61, 6, 1, 52, 43, 47, 45, 46, 40, 53, 59, 56, 1, 19, 56, 43, 51, 47, 53, 8, 0, 19, 53, 42, 1, 57, 39, 60, 43, 1, 63, 53, 59, 6, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"block_size :\", block_size)\n",
        "if debug: print(train_set[:block_size+1])"
      ],
      "metadata": {
        "id": "mK9pCM4VoKVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1628a654-0be7-45ba-a1e2-30de1612fc6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_size : 8\n",
            "[18, 47, 56, 57, 58, 1, 15, 47, 58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is important to understand is that we are going to train our model with various length of context,\n",
        "ie in list of endoded word of size *block_size+1* we are going to get block_size training examples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0kcFXP1ilFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to illustrate that concretly\n",
        "for t in range(1, block_size+1):\n",
        "  print(f\"from this context {train_set[:t]}, we want the model to deduce {train_set[t]}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "for t in range(1, block_size+1):\n",
        "  print(f\"from this context : \\\"{decode(train_set[:t])}\\\", we want the model to deduce \\\"{decode(([train_set[t]]))}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdiHdau5g06q",
        "outputId": "c811d5cf-99ea-43c6-9c23-59f27abca9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from this context [18], we want the model to deduce 47\n",
            "from this context [18, 47], we want the model to deduce 56\n",
            "from this context [18, 47, 56], we want the model to deduce 57\n",
            "from this context [18, 47, 56, 57], we want the model to deduce 58\n",
            "from this context [18, 47, 56, 57, 58], we want the model to deduce 1\n",
            "from this context [18, 47, 56, 57, 58, 1], we want the model to deduce 15\n",
            "from this context [18, 47, 56, 57, 58, 1, 15], we want the model to deduce 47\n",
            "from this context [18, 47, 56, 57, 58, 1, 15, 47], we want the model to deduce 58\n",
            "\n",
            "\n",
            "from this context : \"F\", we want the model to deduce \"i\"\n",
            "from this context : \"Fi\", we want the model to deduce \"r\"\n",
            "from this context : \"Fir\", we want the model to deduce \"s\"\n",
            "from this context : \"Firs\", we want the model to deduce \"t\"\n",
            "from this context : \"First\", we want the model to deduce \" \"\n",
            "from this context : \"First \", we want the model to deduce \"C\"\n",
            "from this context : \"First C\", we want the model to deduce \"i\"\n",
            "from this context : \"First Ci\", we want the model to deduce \"t\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split): #split is either \"train\" or \"eval\"\n",
        "  assert split in [\"train\", \"eval\"], \"split must be 'train' or 'eval'\"\n",
        "  data = train_set if split == \"train\" else validation_set\n",
        "\n",
        "  ix = torch.randint(0, len(data) - block_size, (batch_size,))\n",
        "\n",
        "  x = torch.stack([torch.tensor(data[i:i + block_size]) for i in ix])\n",
        "  y = torch.stack([torch.tensor(data[i + 1:i + block_size + 1]) for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "if debug:\n",
        "  xb, yb = get_batch(\"train\")\n",
        "  print(xb.shape)\n",
        "  print(\"xb:\", xb)\n",
        "\n",
        "  print(\"yb:\", yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99LOhFEtlic0",
        "outputId": "cf00948a-4bb2-4eae-eb26-c9389d897371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "xb: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "yb: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Model"
      ],
      "metadata": {
        "id": "1DjvXjQl4JMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in [\"train\", \"eval\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "YYAWjHQnFsK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, vocab_size) # objective of the embedding, encode the probability for the next token\n",
        "\n",
        "  def forward(self, idx, targets=None): #idx & target are tensor integers of shape (B,T)\n",
        "    logits = self.token_embedding(idx) # shape (B, T, C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss =  None\n",
        "    else:\n",
        "    #refactoring the tensor to match the dimensions required by CrossEntropyLoss\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      #computation of the loss\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tok):\n",
        "    for i in range(max_new_tok):\n",
        "      logits, loss = self(idx)\n",
        "\n",
        "      #focus on the last time step\n",
        "      logits = logits[:, -1, : ] # shape: (B, C)\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "\n",
        "      idx= torch.cat((idx, idx_next), dim=1) #(B, T+1 )\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "m = BigramModel(vocab_size)\n",
        "m = m.to(device)\n",
        "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(idx, max_new_tok=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks_6r6AEyc8R",
        "outputId": "534f3c4b-006f-48d5-e30f-927399a39a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
            "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "\n",
        "#create the optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "6UjDFCshBkEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range (max_iters):\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  #evaluate the loss\n",
        "  if step % eval_interval == 0 :\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\")\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "fZ6wdm4iiB72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx, max_new_tok=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCe7ZoF7i4V8",
        "outputId": "445f1928-1c5f-44e3-bd7d-f38a2082f5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "too id haworo ngos ke or.\n",
            "Thinded HABUKI tourvoer:\n",
            "\n",
            "Howe V:\n",
            "Tor cas,\n",
            "\n",
            "NTEShir ithick;\n",
            "DWe, t endw ch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xmD-t70xRnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
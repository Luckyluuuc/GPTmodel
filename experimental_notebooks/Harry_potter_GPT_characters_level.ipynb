{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import the librairies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "tjrWhgLAgDgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug = True #activate if you want to compute the print/test to see if each step is correctly working\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 2000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(torch.cuda.is_available())\n",
        "eval_iters = 200\n",
        "n_embed = 384 #should be divible by num_head\n",
        "num_heads = 6\n",
        "num_blocks = 6\n",
        "dropout = 0.2\n",
        "fraction_training_data = 0.9\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "5vSmTn7BgqLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87923364-c46a-4916-bfbd-d77b951f558b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7999781c3ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the data\n"
      ],
      "metadata": {
        "id": "FNDDzOU1bMp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn2Obi70aiv9",
        "outputId": "cfdb0ff4-e05d-4e58-cc4a-f4dcab1c05d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-27 13:18:03--  https://raw.githubusercontent.com/amisha-jodhani/text-generator-harry-potter/master/1SorcerersStone.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 444307 (434K) [text/plain]\n",
            "Saving to: ‘1SorcerersStone.txt’\n",
            "\n",
            "1SorcerersStone.txt 100%[===================>] 433.89K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-09-27 13:18:03 (3.02 MB/s) - ‘1SorcerersStone.txt’ saved [444307/444307]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/amisha-jodhani/text-generator-harry-potter/master/1SorcerersStone.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/harry-potter-7-les-reliques-de-la-mort.txt', 'r', encoding='utf-8') as f: #choose the training corpus here\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "jlxPV34vamEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN-7C4EIbcdi",
        "outputId": "5199a3c6-5a4d-4c85-ac77-53e489b2fc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1346803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[2000:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iA2uHWtbi0P",
        "outputId": "f4bf0693-0145-4ecd-c73d-f352fa3fe722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nt le bras gauche dans une sorte de salut et traversèrent la\n",
            "grille comme si le métal sombre n’était qu’un rideau de fumée.\n",
            "\n",
            "Les rangées d’ifs étouffaient le son de leurs pas. Il y eut un bruissement quelque part sur leur droite :\n",
            "Yaxley tira à nouveau sa baguette qu’il pointa par-dessus la tête de son compagnon mais le bruit était\n",
            "dû à un paon, au plumage d’un blanc immaculé, qui s’avançait d’un air majestueux au sommet de la\n",
            "haie.\n",
            "\n",
            "— Il ne se refuse jamais rien, Lucius. Des paons…\n",
            "\n",
            "Avec un petit ricanement, Yaxley remit la baguette sous sa cape.\n",
            "\n",
            "Tout au bout de l’allée, un élégant manoir se dessina dans l’obscurité, des éclats de lumière se\n",
            "reflétant au rez-de-chaussée dans les carreaux des fenêtres à croisillons. Quelque part dans le parc\n",
            "obscur, au-delà de la haie, on entendait le chant d’une fontaine. Des graviers crissèrent sous leurs\n",
            "semelles lorsque Rogue et Yaxley se hâtèrent en direction de la porte qui pivota vers l’intérieur à leur\n",
            "approche, bien qu’apparemment personne ne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a map between characters and integers\n",
        "**Objective**: associate to each characters to an integers using dictionnaries"
      ],
      "metadata": {
        "id": "esV1rfQncxLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "list_char = sorted(list(set(text)))\n",
        "vocab_size = len(list_char)\n",
        "if debug:\n",
        "  print(\"vocab_size :\", vocab_size)\n",
        "  print(list_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JewWArZibmzp",
        "outputId": "9c538c87-ff09-4a69-c647-5951f9f8d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size : 108\n",
            "['\\n', '\\x0c', ' ', '!', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '«', '°', '»', '¾', 'À', 'Â', 'Ç', 'È', 'É', 'Ê', 'Î', 'Ô', 'Ù', 'à', 'â', 'ç', 'è', 'é', 'ê', 'ë', 'î', 'ï', 'ô', 'ù', 'û', 'Œ', 'œ', 'β', '–', '—', '’', '“', '”', '…']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionnary char -> integer\n",
        "stoi = {c:i for i,c in enumerate(list_char)}\n",
        "itos = {i:c for i,c in enumerate(list_char)}\n",
        "\n",
        "if debug:\n",
        "  print(\"stoi :\", stoi)\n",
        "  print(\"itos :\", itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ULJXV_1d85m",
        "outputId": "07ded02f-f23a-4be6-fb3f-70626a0e96d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stoi : {'\\n': 0, '\\x0c': 1, ' ': 2, '!': 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '6': 15, '7': 16, '8': 17, '9': 18, ':': 19, ';': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'Q': 38, 'R': 39, 'S': 40, 'T': 41, 'U': 42, 'V': 43, 'W': 44, 'X': 45, 'Y': 46, 'Z': 47, 'a': 48, 'b': 49, 'c': 50, 'd': 51, 'e': 52, 'f': 53, 'g': 54, 'h': 55, 'i': 56, 'j': 57, 'k': 58, 'l': 59, 'm': 60, 'n': 61, 'o': 62, 'p': 63, 'q': 64, 'r': 65, 's': 66, 't': 67, 'u': 68, 'v': 69, 'w': 70, 'x': 71, 'y': 72, 'z': 73, '«': 74, '°': 75, '»': 76, '¾': 77, 'À': 78, 'Â': 79, 'Ç': 80, 'È': 81, 'É': 82, 'Ê': 83, 'Î': 84, 'Ô': 85, 'Ù': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'Œ': 99, 'œ': 100, 'β': 101, '–': 102, '—': 103, '’': 104, '“': 105, '”': 106, '…': 107}\n",
            "itos : {0: '\\n', 1: '\\x0c', 2: ' ', 3: '!', 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: '9', 19: ':', 20: ';', 21: '?', 22: 'A', 23: 'B', 24: 'C', 25: 'D', 26: 'E', 27: 'F', 28: 'G', 29: 'H', 30: 'I', 31: 'J', 32: 'K', 33: 'L', 34: 'M', 35: 'N', 36: 'O', 37: 'P', 38: 'Q', 39: 'R', 40: 'S', 41: 'T', 42: 'U', 43: 'V', 44: 'W', 45: 'X', 46: 'Y', 47: 'Z', 48: 'a', 49: 'b', 50: 'c', 51: 'd', 52: 'e', 53: 'f', 54: 'g', 55: 'h', 56: 'i', 57: 'j', 58: 'k', 59: 'l', 60: 'm', 61: 'n', 62: 'o', 63: 'p', 64: 'q', 65: 'r', 66: 's', 67: 't', 68: 'u', 69: 'v', 70: 'w', 71: 'x', 72: 'y', 73: 'z', 74: '«', 75: '°', 76: '»', 77: '¾', 78: 'À', 79: 'Â', 80: 'Ç', 81: 'È', 82: 'É', 83: 'Ê', 84: 'Î', 85: 'Ô', 86: 'Ù', 87: 'à', 88: 'â', 89: 'ç', 90: 'è', 91: 'é', 92: 'ê', 93: 'ë', 94: 'î', 95: 'ï', 96: 'ô', 97: 'ù', 98: 'û', 99: 'Œ', 100: 'œ', 101: 'β', 102: '–', 103: '—', 104: '’', 105: '“', 106: '”', 107: '…'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to encode and decode string using dictionnaries\n",
        "\n",
        "def encode(text, stoi = stoi):\n",
        "  list_integers = []\n",
        "  for c in text:\n",
        "    list_integers.append(stoi.get(c))\n",
        "\n",
        "  return list_integers\n",
        "\n",
        "\n",
        "def decode(list_integers, itos=itos):\n",
        "  text = []\n",
        "  for i in list_integers:\n",
        "    text.append(itos.get(i))\n",
        "\n",
        "  text = ''.join(c for c in text) #delete this line if you want a list of char instead of a str\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "if debug:\n",
        "  print(encode(\"hello, I am doing a gpt model\"))\n",
        "  print(decode([5, 48, 19, 23]))\n",
        "  print(decode(encode(\"hello, I am doing a gpt model\")))\n",
        "\n",
        "  print(decode([19]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CGTPP34gSi-",
        "outputId": "1994fd73-843b-4467-dfe5-2b78eb567d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[55, 52, 59, 59, 62, 6, 2, 30, 2, 48, 60, 2, 51, 62, 56, 61, 54, 2, 48, 2, 54, 63, 67, 2, 60, 62, 51, 52, 59]\n",
            ")a:B\n",
            "hello, I am doing a gpt model\n",
            ":\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data and split it into training and validation set"
      ],
      "metadata": {
        "id": "c6wNxlkim7no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tok = encode(text)\n",
        "if debug : print(data_tok[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ_kNZ2rm7XO",
        "outputId": "ed0aa78b-fee2-48b6-f464-1db3cff4030b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 31, 8, 32, 8, 39, 36, 44, 33, 30, 35, 28, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 29, 48, 65, 65, 72, 2, 37, 62, 67, 67, 52, 65, 0, 52, 67, 2, 59, 52, 66, 2, 39, 52, 59, 56, 64, 68, 52, 66, 2, 51, 52, 2, 59, 48, 2, 34, 62, 65, 67, 0, 2, 2, 2, 2, 2, 2, 2, 41, 65, 48, 51, 68, 56, 67, 2, 51, 52, 2, 59, 104, 48, 61, 54, 59, 48, 56, 66, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "z = int(fraction_training_data*len(data_tok))\n",
        "train_set = data_tok[:z]\n",
        "validation_set = data_tok[z:]\n",
        "\n",
        "if debug:\n",
        "  print(\"train set -> length :\", len(train_set))\n",
        "  print(train_set[:100], \"\\n\")\n",
        "  print(\"validation set -> length :\", len(validation_set))\n",
        "  print(validation_set[:100])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfaqDgini9i7",
        "outputId": "ef52ef89-5e3a-4b9d-8884-2b483de62d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set -> length : 1212122\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 31, 8, 32, 8, 39, 36, 44, 33, 30, 35, 28, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 29, 48, 65, 65, 72, 2, 37, 62, 67, 67, 52, 65, 0, 52, 67, 2, 59, 52, 66, 2, 39, 52, 59, 56, 64, 68, 52, 66, 2, 51, 52, 2, 59, 48, 2, 34, 62, 65, 67, 0, 2, 2, 2, 2, 2, 2, 2, 41, 65, 48, 51, 68, 56, 67, 2, 51, 52, 2, 59, 104, 48, 61, 54, 59, 48, 56, 66, 0] \n",
            "\n",
            "validation set -> length : 134681\n",
            "[96, 67, 6, 2, 57, 104, 48, 68, 65, 48, 56, 66, 2, 63, 68, 2, 53, 48, 56, 65, 52, 2, 51, 48, 69, 48, 61, 67, 48, 54, 52, 6, 2, 69, 62, 68, 66, 2, 54, 48, 54, 61, 52, 65, 2, 68, 61, 2, 63, 52, 68, 0, 63, 59, 68, 66, 2, 51, 52, 2, 67, 52, 60, 63, 66, 2, 3, 2, 65, 91, 63, 59, 56, 64, 68, 48, 2, 39, 62, 54, 68, 52, 2, 48, 69, 52, 50, 2, 50, 62, 59, 90, 65, 52, 8, 0, 0, 30, 59, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"block_size :\", block_size)\n",
        "if debug: print(train_set[:block_size+1])"
      ],
      "metadata": {
        "id": "mK9pCM4VoKVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b868234-24a7-4d78-8f8e-dfa69e8899c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_size : 256\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 31, 8, 32, 8, 39, 36, 44, 33, 30, 35, 28, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 29, 48, 65, 65, 72, 2, 37, 62, 67, 67, 52, 65, 0, 52, 67, 2, 59, 52, 66, 2, 39, 52, 59, 56, 64, 68, 52, 66, 2, 51, 52, 2, 59, 48, 2, 34, 62, 65, 67, 0, 2, 2, 2, 2, 2, 2, 2, 41, 65, 48, 51, 68, 56, 67, 2, 51, 52, 2, 59, 104, 48, 61, 54, 59, 48, 56, 66, 0, 2, 2, 2, 63, 48, 65, 2, 31, 52, 48, 61, 7, 27, 65, 48, 61, 89, 62, 56, 66, 2, 34, 91, 61, 48, 65, 51, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 11, 9, 9, 16, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 22, 33, 33, 30, 34, 22, 39, 25, 0, 1, 24, 52, 2, 69, 62, 59, 68, 60, 52, 2, 52, 66, 67, 2, 51, 91, 51, 56, 50, 48, 50, 91, 2, 87, 2, 66, 52, 63, 67, 2, 63, 52, 65, 66, 62, 61, 61, 52, 66, 19, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 78, 2, 35, 52, 56, 59, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is important to understand is that we are going to train our model with various length of context,\n",
        "ie in list of endoded word of size *block_size+1* we are going to get block_size training examples\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L0kcFXP1ilFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split): #split is either \"train\" or \"eval\"\n",
        "  assert split in [\"train\", \"eval\"], \"split must be 'train' or 'eval'\"\n",
        "  data = train_set if split == \"train\" else validation_set\n",
        "\n",
        "  ix = torch.randint(0, len(data) - block_size-1, (batch_size,)) # return a tensor of shape (batch_size) with random values bitween 0 and len(data) - block_size\n",
        "\n",
        "  x = torch.stack([torch.tensor(data[i:i + block_size]) for i in ix])\n",
        "  y = torch.stack([torch.tensor(data[i + 1:i + block_size + 1]) for i in ix])\n",
        "\n",
        "  #y = torch.clamp(y, 0, vocab_size - 1) # not supposed to be necessary but assure that the value are between 0 and vocab_size - 1\n",
        "\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "if debug:\n",
        "  xb, yb = get_batch(\"train\")\n",
        "  print(xb.shape)\n",
        "  print(\"xb:\", xb)\n",
        "\n",
        "  print(\"yb:\", yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99LOhFEtlic0",
        "outputId": "80f50d8c-5e89-4649-a0d2-a1e5c789eb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256])\n",
            "xb: tensor([[66, 66, 56,  ..., 63,  2, 51],\n",
            "        [51, 91, 66,  ..., 49, 59, 90],\n",
            "        [ 0,  0, 26,  ..., 64, 68, 56],\n",
            "        ...,\n",
            "        [61, 67,  2,  ..., 59, 52, 67],\n",
            "        [59, 59, 52,  ..., 50, 59, 48],\n",
            "        [69, 52, 50,  ...,  2, 50, 55]], device='cuda:0')\n",
            "yb: tensor([[66, 56,  2,  ...,  2, 51, 52],\n",
            "        [91, 66, 52,  ..., 59, 90, 60],\n",
            "        [ 0, 26, 59,  ..., 68, 56,  2],\n",
            "        ...,\n",
            "        [67,  2, 52,  ..., 52, 67, 48],\n",
            "        [59, 52,  2,  ..., 59, 48, 61],\n",
            "        [52, 50,  2,  ..., 50, 55, 48]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Model"
      ],
      "metadata": {
        "id": "1DjvXjQl4JMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in [\"train\", \"eval\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = m(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "YYAWjHQnFsK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  # for the forward methods :\n",
        "  # input of size (batch, time-step, channels)\n",
        "  # output of size (batch, time-step, head size)``\n",
        "\n",
        "  def __init__(self, head_size): #head_size = d_k = d_v if we are used to the notation of the original paper\n",
        "    super().__init__()\n",
        "    self.values = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.keys = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.queries = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x.shape : (batch, time-step, channels)\n",
        "    B,T,C = x.shape\n",
        "    key = self.keys(x) #(B, T, head_size)\n",
        "    query = self.queries(x) #(B, T, head_size)\n",
        "    value = self.values(x) # same\n",
        "    energy= query @ key.transpose(-2, -1)\n",
        "    energy = energy/(query.shape[-1]**0.5) # (B, T, T)\n",
        "    energy = energy.masked_fill(self.tril[:T, :T]==0, float(\"-inf\"))\n",
        "\n",
        "    energy = F.softmax(energy, dim=-1)\n",
        "    energy = self.dropout(energy)\n",
        "    out = energy @ value # (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9_X6yj_lbWqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range (num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(x)\n",
        "    out = self.dropout(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "_6rmk_wosq4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(n_embed, 4*n_embed)\n",
        "    self.linear2 = nn.Linear(4*n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "gHH9PL2OHKKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, num_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed//num_head\n",
        "    self.sa = MultiHeadAttention(num_heads, head_size)\n",
        "    self.ffn = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffn(self.ln2(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZWXzzlaYGg5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding = nn.Embedding(block_size, n_embed)\n",
        "    self.attention_blocks = nn.Sequential(\n",
        "        *[Block(n_embed, num_heads) for _ in range(num_blocks)],\n",
        "        nn.LayerNorm(n_embed),\n",
        "    )\n",
        "    self.linear = nn.Linear(n_embed,vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None): #idx & target are tensor integers of shape (B,T)\n",
        "    B, T = idx.shape\n",
        "    tok_embed = self.token_embedding(idx) # shape (B, T, C) with C = n_embed\n",
        "    pos_embed = self.position_embedding(torch.arange(T, device=device)) #(T, C)\n",
        "    x = pos_embed + tok_embed\n",
        "    logits = self.attention_blocks(x) #(B, T, C)\n",
        "    logits = self.linear(logits)#(B, T, vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss =  None\n",
        "    else:\n",
        "    #refactoring the tensor to match the dimensions required by CrossEntropyLoss\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      #computation of the loss\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tok):\n",
        "    for i in range(max_new_tok):\n",
        "      idx_crop = idx[:, -block_size:] #crop\n",
        "      logits, loss = self(idx_crop)\n",
        "\n",
        "      #focus on the last time step\n",
        "      logits = logits[:, -1, : ] # shape: (B, C)\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "\n",
        "      idx= torch.cat((idx, idx_next), dim=1) #(B, T+1 )\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "m = BigramModel(vocab_size)\n",
        "m = m.to(device)\n",
        "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(idx, max_new_tok=100)[0].tolist()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks_6r6AEyc8R",
        "outputId": "e00164e1-5292-4a2b-e928-689531358f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "JR4hlc(W((2’ÇPLÙQÉÔKIêHrXÔhc;;b–b;a )qÉJSàiœpeAà0;0…L.ehFÉY(D;B;yz\n",
            "«bèeVÀβ«IV Ggô u4p6“7SS°?”vÎ?:8Wâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "\n",
        "#create the optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "6UjDFCshBkEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range (max_iters):\n",
        "  xb, yb = get_batch(\"train\")\n",
        "\n",
        "  #evaluate the loss\n",
        "  if step % eval_interval == 0 or step==max_iters-1 :\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\")\n",
        "\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "fZ6wdm4iiB72",
        "outputId": "6be94ffc-573d-4470-c22f-7d89c0438d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7870, val loss 4.7825\n",
            "step 200: train loss 2.3486, val loss 2.3315\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f8c57f75b1a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m#evaluate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {step}: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-03bc64f38780>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.eval()\n",
        "print(decode(m.generate(idx, max_new_tok=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "YCe7ZoF7i4V8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db7e70e-7b2d-413b-ad6b-45ba17c255c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "page de el du le pout lité,-il qui tembritable à l’avait quessi vœurs dant icuner,\n",
            "s’ins s’alloc de hamainsent imprandant : la réparfoité la plume. Lophe tableux poté, gontienstant de\n",
            "bit labrur de main de cour avandé ? n’est ponhé Ron exure à qu’enfillme passer lestêtres un colèverçant avaient fortéraierre hoses du\n",
            "re flats. L’et à L’Harry auconez-imait je queste n’en tose d’aus les. Les latmaint, êtecortages tre et pit ! d’auffinétait derrux poui\n",
            "et Gagidit éclacidés d’une entroureme monsentifiiedore une. Il abloquantt il sudre soy lessi c’inse ser le sembla\n",
            "les. Le dondangris.\n",
            "\n",
            "— Combjette deurny des avaieux l’unes reforts n’avec ! crirsous dortiséplie dans toun melle le nettentemais\n",
            "pas. Les levoirs unir tout motre ?\n",
            "\n",
            "— Alà, lessi, ne penstrait, Grix ! ! ne fonnez noix à pluis ! vachire ne ce s’ése dir vière de pas. Runelletre lats poje\n",
            "des te Chaux dinantioneux seux matres poépoucie survois.\n",
            "\n",
            "— Pusse dert, a moi ci ! Il s’a lont aver vait in sanssienssation déris le son, à sûreveu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xmD-t70xRnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
